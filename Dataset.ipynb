{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5260f255-73df-477a-a002-bab8dd76ad1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### OPEN AQ DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35f85edf-6fa3-4958-93db-82c0bd031eb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "API_KEY = \"beb23f6e525519e691638a5ff7f93fbb2929ea03bb3aeb824eed3b83485cf4f3\"\n",
    "headers = {\"X-API-Key\": API_KEY}\n",
    "\n",
    "# Step 1: Get all locations in India (change country if needed)\n",
    "loc_url = \"https://api.openaq.org/v3/locations\"\n",
    "params = {\"country\": \"IN\", \"limit\": 50}   # fetch up to 50 locations\n",
    "resp = requests.get(loc_url, params=params, headers=headers)\n",
    "\n",
    "if resp.status_code != 200:\n",
    "    raise ValueError(f\"Error fetching locations: {resp.status_code}, {resp.text}\")\n",
    "\n",
    "locs = resp.json()\n",
    "\n",
    "all_records = []\n",
    "\n",
    "# Step 2: Loop through location IDs\n",
    "for l in locs.get(\"results\", []):\n",
    "    location_id = l[\"id\"]\n",
    "    location_name = l.get(\"name\")\n",
    "\n",
    "    latest_url = f\"https://api.openaq.org/v3/locations/{location_id}/latest\"\n",
    "    latest_resp = requests.get(latest_url, headers=headers)\n",
    "\n",
    "    if latest_resp.status_code != 200:\n",
    "        print(f\"⚠️ Skipping {location_id} ({location_name}) due to error {latest_resp.status_code}\")\n",
    "        continue\n",
    "\n",
    "    latest_json = latest_resp.json()\n",
    "\n",
    "    if \"results\" not in latest_json or len(latest_json[\"results\"]) == 0:\n",
    "        continue\n",
    "\n",
    "    for item in latest_json[\"results\"]:\n",
    "        rec = {\n",
    "            \"location_id\": location_id,\n",
    "            \"location_name\": location_name,\n",
    "            \"parameter\": item.get(\"parameter\"),\n",
    "            \"value\": item.get(\"value\"),\n",
    "            \"unit\": item.get(\"unit\"),\n",
    "            \"datetime_utc\": item.get(\"date\", {}).get(\"utc\"),\n",
    "            \"datetime_local\": item.get(\"date\", {}).get(\"local\"),\n",
    "            \"lat\": item.get(\"coordinates\", {}).get(\"latitude\"),\n",
    "            \"lon\": item.get(\"coordinates\", {}).get(\"longitude\"),\n",
    "            \"country\": l.get(\"country\"),\n",
    "            \"city\": l.get(\"city\")\n",
    "        }\n",
    "        all_records.append(rec)\n",
    "\n",
    "# Step 3: Display data\n",
    "if all_records:\n",
    "    pdf = pd.DataFrame(all_records)\n",
    "    df = spark.createDataFrame(pdf)\n",
    "    print(\"✅ Current Data Fetched:\")\n",
    "    df.display()  # show up to 50 rows\n",
    "else:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec38f398-900d-4271-b9b3-66d0f0bb4bf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### NYC DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "007bd745-8d31-49b7-b833-8bebf1abf546",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# NYC 311 API - get the latest 1000 requests\n",
    "url = \"https://data.cityofnewyork.us/resource/erm2-nwe9.json?$limit=1000&$order=created_date DESC\"\n",
    "\n",
    "# Fetch JSON\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "\n",
    "# Convert to Pandas first\n",
    "pdf = pd.DataFrame(data)\n",
    "\n",
    "# Convert Pandas → Spark DataFrame\n",
    "df = spark.createDataFrame(pdf)\n",
    "\n",
    "# Show sample data\n",
    "df.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3796f001-1326-4238-9a3e-7035a81466de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### USGS DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d8bb139-1663-46f7-9ef0-11e30bd4e605",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# USGS API - all earthquakes in the past day\n",
    "url = \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/all_day.geojson\"\n",
    "\n",
    "# Fetch JSON\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "\n",
    "# The earthquake data is under 'features'\n",
    "earthquakes = data['features']\n",
    "\n",
    "# Flatten each feature (properties + geometry)\n",
    "records = []\n",
    "for eq in earthquakes:\n",
    "    rec = {}\n",
    "    # Copy all properties\n",
    "    rec.update(eq['properties'])\n",
    "    \n",
    "    # Add geometry fields\n",
    "    geom = eq.get('geometry', {})\n",
    "    coords = geom.get('coordinates', [None, None, None])  # [lon, lat, depth]\n",
    "    rec['longitude'] = coords[0]\n",
    "    rec['latitude'] = coords[1]\n",
    "    rec['depth'] = coords[2]\n",
    "    \n",
    "    # Add ID\n",
    "    rec['id'] = eq.get('id')\n",
    "    \n",
    "    records.append(rec)\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "pdf = pd.DataFrame(records)\n",
    "\n",
    "# Convert to Spark DataFrame\n",
    "df = spark.createDataFrame(pdf)\n",
    "\n",
    "# Show all columns and sample rows\n",
    "df.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "728937ac-24d7-4951-b37f-7b41d44a1746",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### NEWYOUR TRAFFIC DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d00a80d-3590-4769-9431-343d61319750",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# --- Fetch live Chicago traffic congestion data ---\n",
    "url = \"https://data.cityofnewyork.us/resource/5uac-w243.json?$limit=1000\"\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "\n",
    "# Check if data is returned\n",
    "if len(data) == 0:\n",
    "    print(\"No data returned from the API!\")\n",
    "else:\n",
    "    # --- Convert to Pandas DataFrame ---\n",
    "    pdf = pd.DataFrame(data)\n",
    "\n",
    "    # --- Convert to Spark DataFrame ---\n",
    "    df = spark.createDataFrame(pdf)\n",
    "\n",
    "    # --- Add ingestion timestamp ---\n",
    "    df = df.withColumn(\"ingest_time\", current_timestamp())\n",
    "\n",
    "    # --- Show sample rows and schema ---\n",
    "    df.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b62f99d1-71ae-488e-8d98-c7ddb112c77f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### DOT Traffic Speeds NBE Updates every minute\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b339faa-4409-4c58-bf36-edf40231dccf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "# Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# --- Step 1: Fetch DOT Traffic Speeds NBE ---\n",
    "url = \"https://data.cityofnewyork.us/resource/i4gi-tjb9.json?$limit=1000\"\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "\n",
    "# --- Step 2: Convert to Pandas DataFrame ---\n",
    "pdf = pd.DataFrame(data)\n",
    "\n",
    "# --- Step 3: Convert to Spark DataFrame ---\n",
    "df = spark.createDataFrame(pdf)\n",
    "\n",
    "# --- Step 5: Show sample rows and schema ---\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee11cfbb-f81e-4465-a20c-39339f13f26d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "# Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# --- Step 1: Fetch DOT Traffic Speeds NBE ---\n",
    "url = \"https://data.cityofnewyork.us/resource/n6c5-95xh.json?$limit=1000\"\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "\n",
    "# --- Step 2: Convert to Pandas DataFrame ---\n",
    "pdf = pd.DataFrame(data)\n",
    "\n",
    "# --- Step 3: Convert to Spark DataFrame ---\n",
    "df = spark.createDataFrame(pdf)\n",
    "\n",
    "# --- Step 5: Show sample rows and schema ---\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70df57c1-bc8a-4912-9d5e-a6d1104df62d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Dataset",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
